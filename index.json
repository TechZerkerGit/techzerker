[{"categories":null,"content":"Zettelkasten Method Imhoff ","date":"2022-01-15","objectID":"/zettelkasten/202007262035-zettelkasten-method-imhoff/:0:0","tags":"Zettelkasten","title":"202007262035","uri":"/zettelkasten/202007262035-zettelkasten-method-imhoff/"},{"categories":null,"content":"202007262035 Stefan started Zettelkasten in 2019, and in his case found the Mac/iOS Devonthink software fit his design best. Stefan does not do regular #Tags, as his focus on Devonthink as the software, it does the equivalent of tags, but apparently smarter. Stefans version of workflow into his Zettelkasten is to have a handful of Inboxes to gather rough notes, things to reference and ideas. He then generally processes the inbox once a day into useful notes. Stefan had excellent rules listed for selecting privacy focused software. ","date":"2022-01-15","objectID":"/zettelkasten/202007262035-zettelkasten-method-imhoff/:1:0","tags":"Zettelkasten","title":"202007262035","uri":"/zettelkasten/202007262035-zettelkasten-method-imhoff/"},{"categories":null,"content":"Links Zettelkasten-Method ","date":"2022-01-15","objectID":"/zettelkasten/202007262035-zettelkasten-method-imhoff/:1:1","tags":"Zettelkasten","title":"202007262035","uri":"/zettelkasten/202007262035-zettelkasten-method-imhoff/"},{"categories":null,"content":"References Zettelkasten — How One German Scholar Was So Freakishly Productive Living with a Zettelkasten #Zettel #Notes #Zettelkasten | ","date":"2022-01-15","objectID":"/zettelkasten/202007262035-zettelkasten-method-imhoff/:1:2","tags":"Zettelkasten","title":"202007262035","uri":"/zettelkasten/202007262035-zettelkasten-method-imhoff/"},{"categories":["Android","Mobile"],"content":"After reading Digital Minimalism several times when it was released, I looked for ways to implement the ideas presented. They ranged from minor changes to the existing smartphone all the way to the realm of no mobile device. The first serious solution I settled on was using an LTE Tablet in place of the smartphone, circa 2019. ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:0:0","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"The Setup ZTE Grand X View 3 8” Tablet (LTE) Ooma VOIP Service w/Premium Plan Virgin Mobile Data Plan ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:1:0","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"ZTE Tablet The ZTE Grand X View 3 was selected, as I was targeting a budget device. I wasn’t ready to commit to the cost of an iPad Mini, and was focusing on smaller tablets to remain semi-portable. In that range, this ZTE Tablet had the highest resolution screen and more memory than most other LTE 7-8” Tablets that supported Canadian carriers. Cost: $6/mo - Two Year Contract ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:1:1","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"Ooma VOIP Service I shopped around for a few VOiP providers before selecting Ooma. One of my key criteria was porting my existing number. The only provider that came back with that option was Ooma. To seal that deal, Ooma offered mobile apps that would allow all calling features with their premier plan. Cost: $10/mo - Paid Yearly ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:1:2","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"Virgin Mobile Data Plan The selection of the carrier came down to the device selection, as I wasn’t interested in buying up front. In this case, the core carriers all had more outdated entry level small Android tablets, for the same price that Virgin had the ZTE. In addition, Virgin had a decent offer of 4 GB data for $15/mo. At the time of purchase, other carriers were all running scaling tablet plans that started at $5 for 100mb, and quickly scaled to $20 for 2 GB and up. Cost: $15/mo ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:1:3","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"Total Solution Cost These components together formed the plan, and would double up as a budget savings vs the $75-100/mo standard smartphone plan. Total Cost: $31.00/mo ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:1:4","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"Why Use a Tablet as a Phone? The thought process to this solution was to maintain the benefits of the smartphone, but introduce a barrier to the always in your pocket easy access. The concept being that I could have access to apps on the go, and features like navigation. I would be able to get calls while travelling and be in contact, with limits. The goal was that while out and about daily, this tablet could remain in the vehicle console, in a laptop bag and sometimes just stay home on the coffee table. It would mean having mobile capability, with enough impediment to minimize habitual time-wasting usage. ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:2:0","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"How Did It Work? After about eight months with this deployment, I would rank it as a success. My mobile device usage went way down, but I could still be reached the vast majority of the time. Coming from the coming-of-age era of home phone with answering machine, I was more reachable than that, with limitations. Did it have complications? Absolutely! Phone call usage meant always having earbuds with a mic available, otherwise I would have to obnoxiously use speakerphone in public. I had a handful of times in stores that I wanted a quick picture, or wanted to text my spouse a question, but the tablet was in the vehicle. However, none of these were serious problems. Battery life was excellent, the LTE speeds were as fast as any device, and the larger screen was a plus. The most noticeable drawback was being VoIP, it has no support for SMS. This had little impact for me, as I operate more on Telegram. A few financial services have given issue due to only supporting SMS for two factor, instead of my preferred TOTP options, so those took some hoops to jump through. Regarding call quality itself, Ooma does the job as well as most home phone VoIP services. On decent LTE, I could be heard, with the exception of call centers, which either struggled to hear me or had a very long lag time. But in all honesty, most of those call centers have had issues with my work Bell service when it is on wifi calling mode, so I’m not knocking it against Ooma. ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:3:0","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Android","Mobile"],"content":"Conclusion I believe using a VOIP service on a cellular tablet as a viable option. It does have its drawbacks, but then, that was the point being inspired by digital minimalism ideals. The cost savings vs a full cellular plan (in Canada at least) were obvious, and I still had access often enough to apps I might need on the road. I still partially have the service, both the tablet and the Ooma service are active, but I also carry an iPhone provided by my workplace. As such, the Ooma app for my personal number is installed and works great on both devices. It’s an option worth considering in my books! ","date":"2021-12-29","objectID":"/2021-12-29-tabletphone/:4:0","tags":["Tablet","Phone","”Android”","”Mobile”","VoIP"],"title":"Using LTE Tablet as a Phone","uri":"/2021-12-29-tabletphone/"},{"categories":["Tech","Writing"],"content":"My love for small, personal blogs and writing that is not SEO or advertising focused has always been strong, mostly calling back to my 90’s internet roots. From the era before the big central platforms like Facebook, when we all had independant sites, free GeoCities blogs, and the like. In that world, which still exists today, albeit smaller, easy quick sharing and likes did not exist. This meant that in most cases, as far as any specific blog author knew, you were writing into a void that no one was reading, which can still be the case. The inspiration for this little commentary was the recent events as I started returning to some writing (and coding) plans. This site is built with the Hugo framework, and hosted via GitHub Pages. When I returned to start writing a bit of a history and way forward artcile on trying to get back into Coding, and tackling a #100DaysofCode challenge, I found my GitHub Actions tasks that publish these posts were failing. Hours of searching and mucking about, and the action workflow that had previously worked just kept failing, and every element of documentation I could find for the most common workflow scripts, pointed to the same failing solutions. Because these most common solutions relied on importing other components/scripts, I was not getting decent or useful error logs to explain why the failure was occuring. I know there are methods to further pull those workflows and components apart to troubleshoot, but I wanted to keep searching, and that’s where another personal blog, more or less shouting into the void of the internet, solved my problem. In my searching, results came up pointing to a personal blog Matt Harrison, a fellow advertising free, Hugo built and GitHub Pages hosted blog. Like myself, the writing is sometimes sporadic because it’s not designed as a source of income directly, and it’s not built to encourage agressive likes/favorites/shares. The article itself was Automating this Hugo Blog with GitHub Actions. Matt gave a well written summary of GitHub actions, and then provided his own actions workflow script. The main script was followed by a nice breakout summary of the key stages of the script and what they were doing, which solidified by understanding of the process. Because it did not rely on other pre-build actions, beyond the standard git checkout action, it was easy to understand what was going on, and it solved my own problems perfectly! For my use case, the only change I had to select a different Hugo version required for my theme, as well as update the curl download to pull Hugo Extended, also required for my theme: steps: - name: Install Hugo env: HUGO_VERSION: 0.86.1 run: | mkdir ~/hugo cd ~/hugo curl -L \"https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_Linux-64bit.tar.gz\" --output hugo.tar.gz tar -xvzf hugo.tar.gz sudo mv hugo /usr/local/bin Once I realized this solved my problems, and I could get back to writingi, I was excited. I reached out to Matt to give a quick Thank You for his posted solution, and recieved a pleasant response back equally agreeing that often this type of writing feels like shouting into the void. So hey, if you’re reading a small site like this, especially one not pushing advertising and tracking, and you find it useful, reach out to the author and let them know it helped, it’s always appreciated. If you are looking for more reading, I maintain my own Blogroll listing here on the site, mostly generated from my MiniFlux RSS Feed. Some of the entries in other categories are larger sites with tracking and advertising, but first on the list is personal Blogs that I like to follow, and I occasionaly update the list when I find new entries. In the next few days, I’ll get onto that planned article (now that everything is working) on my development history/education, and my plans for the future. ","date":"2021-10-21","objectID":"/2021-10-21-blog-community/:0:0","tags":["Blog","Community","Writing"],"title":"Personal Blogs and Community Support","uri":"/2021-10-21-blog-community/"},{"categories":null,"content":"Thanks for reading TechZerker! I’m Scott, a Canadian tech professional in the industry since 2005. I have been passionate about technology long before it became my profession, starting off with an Apple IIC back in the early 90’s! Portrait created by: Peter @ Rock Paper Cynic Over time I've evolved from a regular tech user on the windows flavor of the day, to a die-hard windows fan (including Windows Phone!). After that mess, I finally have settled comfortably into open source and the linux world for my needs, including gaming on Linux full time. When it comes to gaming, I am a mix between modern-ish gaming and what is best described as nostalgic gaming (late 90's and 2000's), which rarely counts as *retro*, at least not yet. I created TechZerker as my source to talk about these technology, gaming and linux subjects. I have since expanded it as a personal blog to cover a variety of interests, as despite working in IT, I have a variety of non-IT interests from History, to Stoicism, Classic Cars and Motorsports, and more. Given my career, most of what I write about is related to technology, but felt it worthwhile to demonstrate we’re not all hackers or basement dwellers who live and die by a terminal. Despite these linux leanings, I am also a happy iOS/Apple user, as I also believe in the core philosiphy of use what works for you. As a final point, I am definitely concerned at the modern smartphone addiction challenges, and as I’m lifting my head up, I can’t un-see a world scrolling through feeds, likes and shares with no real interaction, so you won’t find those features here, and instead I encourage genuine and intelligent discussion on topics. Here are the most common ways to reach me: Mastodon: @TechZerker@fosstodon.org Telegram: @TechZerker Twitter: @TechZerker ","date":"2021-10-13","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Site Navigation A few short notes about some of the sections on this site, beyond the main regular posts and articles: ","date":"2021-10-13","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Blogroll As was common in the earlier days of the web, when it was more open and less platform centralized, I’ve added a Blogroll page. The Blogroll is a quick listing of various blogs and other sites I follow and find interesting. The intent from the era of Blogrolls is if you come my way because of something I wrote, you can organically see others I follow, and find other similar interests without an algorithm telling you who ( is most profitable) for you to see or follow. As I find other writers and organizations that interest me, I’ll update the Blogroll with new entries, and likely write a post about any new additions to highlight them. ","date":"2021-10-13","objectID":"/about/:1:1","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Links The links section I added was originally going to be just a flat page of interesting websites and content, but that was going to be too similar to the Blogroll. In my usage, it is a focus on one specific category, as far as how the site is designed, of posts. When I find an interesting article that I want to share, where most would re-share, with or without commentary, to a service like Facebook or Twitter, I write a short couple sentances on why I’m sharing this link, as a regular post. The Links page is just a filtered list of all that type of content, to make it easy to seperate from anything written directly by myself. It’s a good section to just browse if your looking for a wider range of material than just my own writing! ","date":"2021-10-13","objectID":"/about/:1:2","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Reading Inspired by a few other blogs I follow, I liked the idea of having a basic public reading list, as another way to share and collaborate. It makes for an easy location to show what I am currently reading, what I have read in the past (although that will take time to backfill!), and also a collection of all the “want to read” selections that I’ll add to as I see recommendations elsewhere. If it helps encourage me to improve my reading habits, great! Otherwise, if it helps you expand your own reading lists or discover new authors to follow and support, that is the ultimate goal. Where I can, my intention on the reading list is to link out as often as possible, to non-Amazon stores where you can find many of these books, not necessarily to be anti-amazon, but at least to give some exposure to competitors and give them a fighting chance vs. the gorilla in the book-selling room. ","date":"2021-10-13","objectID":"/about/:1:3","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Privacy \u0026 Advertising As part of operating this site, originally started via Write Freely and it’s ideals, it has no Likes, Hearts or Quick Shares. You can contact me above for a genuine discussion. For privacy, it has no identity tracking or data gathering mechanisms. I use Duck Duck Go’s tools and regularly check my own site to make sure I have not unintentionally violated this. The only type of advertising you will ever find here will be direct old fashion referral links in plain text. These links will only be present if it flows well with the writing as to not disrupt the reading experience. If any of those referrals ever actually turn a profit, I will always make sure a portion goes back to donations to open source projects like WriteFreely.org. ","date":"2021-10-13","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Tech","Linux","Links"],"content":"The Linux For Everyone channel, founded by Jason Evangelho today posted an excellent commentary, written by Alan Diggs on the recurring challenges of the toxicity in the Linux community. While there are some amazing and supportive groups throughout the Linux community, it is easy to agree with this commentary that the toxicity is a scar that consistently hurts the overall image of Linux and Open Source. Working in enterprise IT, I personally know a selection of working IT folks who at my encouragement have tried Linux, and if they couldn’t reach me, gone to a forum for questions, and then texted me quitting Linux because of how agressivly they were berated. My core approach with Linux and encouraging adoption has been to talk about my own system and what works (and what does not) and show it off a little. If anyone reaches out and shows some interest, I will gadly answer what questions I can, understand their needs, and attempt to help and steer them in the right direction. However, the core philosphy is: Use what works for you. Period. If your Windows 10 system is getting the job (or game) done for you, and anything the community has shown in Linux (or BSD) has not grabbed your attention, that’s fine, live and let live. If your interested in the systems the positive users in the community talk about and demonstrate, reach out with questions. If you’ve tried before and hit a “Wall of Toxicity”, then here are a few great places to join and ask questions where we’ll be friendly and helpful, regardless of your experience level: Fosstodon (Mastodon Instance) Telegram: Linux For Everyone ","date":"2021-02-11","objectID":"/2021-02-11-toxic-linux/:0:0","tags":["Linux","Community","Links"],"title":"Toxicity in Linux Community (Link)","uri":"/2021-02-11-toxic-linux/"},{"categories":["Links"],"content":"This article is a concise summary on RSS, and why it is a great way to syndicate content. It also hits the nail on the head for why it’s being “killed” (marked as dead) by the content providers most are familiar with. (Twitter, Facebook, etc.) In short, with RSS, everything is chronological from when it was published, no algorithims deciding what you should read. Generally, when a site I have followed has removed RSS, I don’t intentionally stop reading it (aka: I’m not boycotting it), it mostly just falls off my radar, as there are very few sites I directly visit on a regular basis. I used RSS via Google Reader heavily when it was popular, that being pre or early Facebook days. When it was shutdown, I moved onto Feedly, before finally rolling my own Self-Hosted MiniFlux RSS reader instance. Similar to Ibrahim, my site built with Hugo has RSS available and encouraged. Article: With RSS, You are in Control ","date":"2020-12-08","objectID":"/2020-12-08-rss-in-control/:0:0","tags":["Links","RSS","MiniFlux"],"title":"RSS: Controlling Your Feed (Link)","uri":"/2020-12-08-rss-in-control/"},{"categories":["Links","Tech"],"content":"Through an old article in my RSS, I came across this very long twitter post. I found it worth sharing as it expands one of the directions tech is moving in, rapidly. Will we all welcome it? Will it blend well with FOSS? Will it run EMACS or VIM?!? … Source Twitter Post Copied in full below for easier reading Paperspace and Rollapp have my head spinning. It’s so clear this is the future and so clear hardly anyone sees it. Enterprise has an emerging model called Hyper-Converged Infrastructure (HCI) which is basically: the compute, the storage, the display, the delivery, and even the software don’t exist until you need them. What that means is: given a huge data center, you can choose from whatever resources are available at the moment and run a streaming software experience to the end user by connecting them all on-the-fly. So a processor in Denver and storage in Albuquerque can simulate a single app experience in Sheboygan. The hypervisor makes sure it’s all seamless by containerizing everything. So it’s on-the-fly software legos. Currently, software expecting to run on a desktop is virtualized and runs in this world without knowing it. The next step is, now we can write software than anticipates this environment. When you think of Notion or Airtable, they are basically interfaces to a database. Data sits in blocks (chunks of markdown and JSON) and has so structure until it’s displayed. Almost like how a 3D game – there are granular resources which are rendered at run-time. Also like a 3D game, if something is off-screen, it isn’t rendered. If something is in a different database, it isn’t accessed. So the app itself can be extremely lightweight and delivered in a ton of formats. A table can quickly become a spreadsheet, or a kanban, or a wiki article. It’s just a different way of displaying the blocks. In a hyperconverged world, the blocks aren’t even in the same database. They could be anywhere. They could be anything. They could have a CPU assigned to them– each cell in a spreadsheet running in a different data center. This used to be insanely difficult, but now it’s not even expensive. Paperspace costs about 1/10th of a penny per minute, and that’s -marked up-. By comparison, a MacBook Air costs about 6x as much, if you use it 8 hours a day, every day. So now you can assign an entire computer to a single cell in a spreadsheet, and assign another computer to unify all of the calculations happening across the cells. But no one is designing software like this currently. It’s not hard to think of a different computer handling part of a task – xBox One supposedly had cloud rendering, but it didn’t do much. Google Stadia runs games inside a hyper converged environment, but they’re all currently games designed to expect to be run on a single desktop. The next wave in personal computing is going to come when we have 5G, because then we’ll know we can expect there to be fiber-speed connectivity under all conditions. Then we’ll know that we can have any part of an application rendered anywhere. Instead of embedding gifs in a tweet, you could embed a whole game. And it’ll take zero time to load. Any machine learning application can have unlimited processing power to computer, because the results will be streamed from special purpose chips in the cloud. Suddenly carrying around the fastest available processor in your pocket, only to have it be powered down 90% of the day, will seem obsurd. Rather than push 5nm CPUs to the edge devices, it’ll make way more sense to fill semi trailers with them and leave them every few miles parked next to a fiber line and a power pole. “Computers” will become just screens. There will be nothing to upgrade, except the modem. Your “desktop” will be a Chromecast-sized puck with a 5g SIM card. Who will create the apps for this new future? No-code will be the only way to build, with entire apps abstracted into a single drag and drop bundle of code; the complexity of building apps will be so abstracted, devel","date":"2020-06-08","objectID":"/2020-06-08-paperspace/:0:0","tags":["Links","PaperSpace","Cloud","Computing","Future Tech","Infrastructure","Stadia"],"title":"PaperSpace and Future Hyper-Converged Computing (Link)","uri":"/2020-06-08-paperspace/"},{"categories":["Links"],"content":"This story already circulated plenty in months past, but I decided to link it today after a good co-worker discussion on who writes history, and always being willing to learn facts. In our case, our discussion started with referring to Hunting Hitler, and understanding at the close of the war, people needed to hear we got him!, even if it wasn’t accurate, and he was actively being searched for. The same goes for Japan in 1945, the American public, at least in the short term, needed to hear that the Atomic Bomb developed at great expense and sacrifice ended the war. This article is a great read to remind us all to read and analyze facts for ourselves, and remember who writes history. The Bomb Didn’t Beat Japan … Stalin Did (Foreign Policy) ","date":"2020-06-07","objectID":"/2020-06-07-japan-atomic-bomb/:0:0","tags":["Links","History","Japan","WWII","Atomic Bomb"],"title":"The Bomb Didn't Beat Japan (Link)","uri":"/2020-06-07-japan-atomic-bomb/"},{"categories":null,"content":" I primarily read other content via my RSS Feed Reader, which is a Self-Hosted Miniflux server. While a Blogroll traditionally is just Blogs, I have expanded my list to anything in my RSS that I would like to share. A good majority given my career is tech blogs and tech news, but I have included a variety of other categories as well for reader interest. ","date":"2020-06-04","objectID":"/blogroll/:0:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Blogs Bringing back the 90s Internet (Feed) iDiallo.com (Feed) jlelse’s Blog (Feed) Kev Quirk (Feed) Marko Saric (Feed) Posts on fribbledom’s Journal (Feed) Quick Brown Fox by Salman Ansari (Feed) C’est la Z (Feed) Robert Heaton | Blog (Feed) Salman Ansari (Feed) The New Oil (Feed) zerokspot.com (Feed) Matt Harrison fs Blog ","date":"2020-06-04","objectID":"/blogroll/:1:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Comics COMMISSIONED comic (Feed) Ctrl+Alt+Del Comic (Feed) Dilbert (Feed) Garfield (Feed) Garfield Classics (Feed) O Abnormal’s art barf (Feed) Rock, Paper, Cynic (Feed) The Adventures of Business Cat (Feed) Frogpants Studios (Feed) ","date":"2020-06-04","objectID":"/blogroll/:2:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Current Events \u0026 News Wikipedia: Portal: Current events (Feed) ","date":"2020-06-04","objectID":"/blogroll/:3:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Finance \u0026 Money Unclutterer (Feed) Clever Dude Personal Finance \u0026 Money (Feed) Dumb Little Man (Feed) Get Rich Slowly - Personal Finance That Makes Sense. (Feed) Man Vs. Debt (Feed) The Simple Dollar (Feed) ","date":"2020-06-04","objectID":"/blogroll/:4:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Gaming GamingOnLinux Latest Articles (Feed) Polygon (Feed) Punished Props Academy (Feed) ","date":"2020-06-04","objectID":"/blogroll/:5:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Linux GamingOnLinux Latest Articles (Feed) Endeavour OS Discovery (Feed) It’s FOSS (Feed) ","date":"2020-06-04","objectID":"/blogroll/:6:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Health \u0026 Fitness Nerd Fitness » Blog (Feed) ","date":"2020-06-04","objectID":"/blogroll/:7:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Information Security Krebs on Security (Feed) An Analytical Approach (Feed) Graham Cluley (Feed) Naked Security (Feed) Threatpost (Feed) Webroot Blog (Feed) ","date":"2020-06-04","objectID":"/blogroll/:8:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Life \u0026 Society Aeon (Feed) Bored Panda (Feed) Danielle LaPorte: white hot truth + sermons on life (Feed) Darius Foroux (Feed) Longform (Feed) Mark Manson (Feed) Seth Godin (Feed) ","date":"2020-06-04","objectID":"/blogroll/:9:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Manliness Daily Stoic (Feed) Stories by Ryan Holiday on Medium (Feed) The Art of Manliness (Feed) Petrolicious (Feed) StanceWorks (Feed) ","date":"2020-06-04","objectID":"/blogroll/:10:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Tech News ArsTechnica (Feed) Motherboard (Feed) Recode (Feed) TechCrunch (Feed) ","date":"2020-06-04","objectID":"/blogroll/:11:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Podcasts Daily Tech News Show (Feed) The Morning Stream (Feed) CurrentGeek! (Feed) THE BOOP SHOW! (Feed) There Will Be Dungeons! (Feed) Choose Linux (Feed) Linux Action News (Feed) Linux Unplugged (Feed) Self-Hosted (Feed) Late Night Linux (Feed) ","date":"2020-06-04","objectID":"/blogroll/:12:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"Writing iA (Feed) Write To Done (Feed) ","date":"2020-06-04","objectID":"/blogroll/:13:0","tags":null,"title":"Blogroll","uri":"/blogroll/"},{"categories":null,"content":"After seeing a few blogs and sites I follow add Reading lists, showing what they are, have previously and would like to read, I liked the idea! So here we are, right now, the “have read” list is short until I can scrape my memory for what has past, as I have not previously kept a written record. The tail end of the list will be my running wish list in my reading and travels of books I would like to read. Hopefully maintaining a bit of an online record will help improve my reading habit a bit! (I love reading, but tend to get distracted…) ","date":"2020-06-04","objectID":"/reading/:0:0","tags":null,"title":"Reading","uri":"/reading/"},{"categories":null,"content":"Reading Now Title Author Winner Take Nothing Ernest Hemingway Dead Doubles Trevor Barnes Digital Minimalism Cal Newport ","date":"2020-06-04","objectID":"/reading/:1:0","tags":null,"title":"Reading","uri":"/reading/"},{"categories":null,"content":"Finished Reading Title Author Finished Commentary? Everything Is F*cked Mark Manson 2020-06-06 \u003c–\u003e ","date":"2020-06-04","objectID":"/reading/:2:0","tags":null,"title":"Reading","uri":"/reading/"},{"categories":null,"content":"Would Like To Read Title Author Deep Work Cal Newport How to Live (Preface to Montaigne) Sarah Bakewell Montaigne Stefan Zweig The Moviegoer Walker Percy The Laws of Human Nature Robert Greene The Way of the Intelligent Rebel Olivier Roland Leisure: The Basis of Culture Josef Pieper So You’ve Been Publicly Shamed Jon Ronson The Greatest Empire: A Life of Seneca Emily Wilson Lincoln’s Virtues William Lee Miller Tyrant: Shakespeare on Politics Stephen Greenblatt Tiger Woods Jeff Benedict \u0026 Armen Keteyian Clementine: The Life of Mrs. Winston Churchill Sonia Purnell Blue Ocean Strategy W. Chan Kim \u0026 Renee Mauborgne Blue Ocean Shift W. Chan Kim \u0026 Renee Mauborgne Memoirs of Hadrian Marguerite Yourcenar Meditations Marcus Aurelius How to Be Free Epictetus Essentialism Greg McKeown Up From Slavery Booker T. Washington Them: Why We Hate Each Other–and How to Heal Ben Sasse The Death of Expertise Tom Nichols Reasons and Persons Derek Parfit Range: Why Generalists Triumph in a Specialized World David Epstein Average Is Over Tyler Cowen Speed \u0026 Scale John Doerr Disclosure: Reading list title links utilize text based affiliate links, as per my Privacy \u0026 Advertising policy (About page). I am activly cultivating better affiliate sources such as Indigo to replace Amazon, as I would like to pull away from Amazon services, however, many of the better affiliate systems need to see higher traffic in order to be approved. Whenever possible, if I have an affiliate link I will put preference to a non-Amazon provider if available. ","date":"2020-06-04","objectID":"/reading/:3:0","tags":null,"title":"Reading","uri":"/reading/"},{"categories":["Tech","Linux"],"content":"It is no secret that my favorite distro for Linux after much trial and error, landed on Arch Linux. I found I prefer the rolling release model vs major version upgrades and the AUR (Arch User Repository) is incredible for finding and installing packages. That being said, it’s biggest win is the Arch Wiki. I find however, that no matter how often that is repeated in the Arch circles, you still find forums full of solutions that the Arch Wiki covers better, or even conflict the Wiki. I wrote this today, because I saw that example again in searching, where probably due to some previous mistake, when I went to update my system today, I was getting a failure for a pair of conflicting packages, which prevents updating as part of Arch giving you a chance to make sure you know what your doing… A search for Arch Pacman + Conflicting Files returns all sorts of results, from various BBS Sites, Reddit, and Forums, with the Arch Wiki nestled tightly among them. However, in a rather large number of the forums I took a quick browse at, were suggestions like: pacman -Syuf and pacman -Suf, among others, which most importantly are encouraging the f force flag. In some cases, other responses speak up right away much as I am, and suggest not taking that approach, as it’s a very strong-handed and dangerous approach to updating an Arch system. Sure enough, in a bunch of cases (not a scientific sampling), there are follow-up posts of panic stricken users, who range from system won’t boot to pacman is completely broken, and other results. Where a quick search for this same problem to the Arch Wiki, leads you down the Pacman page to it’s troubleshooting section. The core solution detailed: This is happening because pacman has detected a file conflict, and by design, will not overwrite files for you. This is by design, not a flaw. The problem is usually trivial to solve. A safe way is to first check if another package owns the file (pacman -Qo /path/to/file). If the file is owned by another package, file a bug report. If the file is not owned by another package, rename the file which ‘exists in filesystem’ and re-issue the update command. If all goes well, the file may then be removed. Sure enough, in my case, I ran pacman -Qo /usr/lib/thefileinquestion, and it returned that no package owned that file. So following the wiki, I did a quick rename of that file (not delete, in case it does end up being needed!). I re-ran my update process, no conflicts, and everything appears to be working! Simple :) In the case of Arch, when in doubt, start with the Arch Wiki, and then ask for elaborations on those solutions if they are not working, or not solving your problem, and you likely will save yourself a lot of headache! I even went so far for a while, as to run one of my self-hosted servers on Vultr on Arch, given they allowed custom ISOs! It is not always the best solution for a server, depending on the applications running, but its worth evaluating, and again, it’s hard to fault the documentation power of the Wiki. ","date":"2020-04-24","objectID":"/2020-04-24-arch-wiki-conflicts/:0:0","tags":["Linux","Arch","Arch Linux","Arch Wiki","AUR"],"title":"Arch Linux – Conflicting Files and the Arch Wiki","uri":"/2020-04-24-arch-wiki-conflicts/"},{"categories":["Tech","Linux"],"content":"While all my personal systems are exclusively running Linux, as is the nature of working in most IT Support roles, the base of my shared company workstation in the office is Windows 10. After a bunch of article reading, research and testing, this is a quick summary of what I use to have what has worked for me as a fully functioning i3 graphical desktop, running via WSL (Windows Subsystem for Linux) on a functioning X-Server. For me at least, I’ve found it works much better than when I tried to have a VM running on the workstation, as it’s far from new or high performance. I won’t re-hash instructions that are widely available for getting the base WSL system installed and running, as it’s pretty straight forward to get WSL feature enabled, and then go through the process to in my case, setup the Ubuntu WSL version straight from the Windows Store. Once you have that up and running, so that you can get to your basic WSL command prompt… Obviously in my example, I’ve added a few things to my .bashrc, and in windows I’ve told it to let this window be a bit transparent. Next up, you need a functioning X Server on windows that we can use to create the display. I tried a few, but in the end had the best luck with VcXsrv, which you can download from that link at SourceForge. No special instructions needed beyond getting it installed, ideally in the default paths. Once that is installed, you can also go ahead and install your DE (Desktop Environment) in your WSL install. In my case, because it’s my preference and it’s light weight, I stuck with i3, so that’s what I’ve tested here. That means your mileage may vary with other DE’s. No special considerations for the install, in this case it was a standard apt install i3 on a fully updated system. Now comes the fun part of pulling it all together, these scripts are dirty and simple, I guarantee someone could write them to be cleaner or better looking, but they were written just for myself originally to this end. On my desktop I have: wsl.vbs ' This script is meant to be launched from the Windows side, to start up a decorationless ' VcXsrv container for the environment. ' ' You may need to change this to reflect your VcXsrv install path as well as screen resolution. ' Then after the VcXsrv container is running, it pulls the WSL Ubuntu into it, along with a launch script. Set shell = CreateObject(\"WScript.Shell\" ) shell.Run \"\"\"C:\\Program Files\\VcXsrv\\vcxsrv.exe\"\" :0 -screen 0 @1 -ac +xinerama -engine 1 -nodecoration -wgl\" WScript.Sleep 200 shell.Run \"ubuntu -c \"\"~/.scripts/wlaunch\"\"\", 0 Following that, before this actually works, as you can see, inside my WSL Ubuntu home directory, I’m calling a script called wlaunch: wlaunch #!/bin/bash # meant to be run with `bash -c \"/path/to/wlaunch\"` when running from e.g. a Windows shortcut # explicitly needed when launching with bash -c from Windows source ~/.bashrc export DISPLAY=:0 i3 Obviously, replace the last line calling i3 with something else if you are using a different environment, like XFCE, which I have not tested. When that’s all in place, all things being equal, you should be able to run wsl.vbs, and after a few seconds, be staring at your desktop environment! In my case, because its i3, I have beyond that heavily setup my own i3 config files, polybar, etc. for the look I want: After more than a year of this setup, I have yet to have any issues with any program. I’ve run things like Sublime Text, Firefox and Spacemacs all within this desktop environment without any issues, and with way better performance than I had on a VM. For easy workflow between the base Windows 10 system and this WSL i3, I’ve simply created symlinks for my core folders like Documents and my mapped network drive, given all drives in WSL1 are available at /mnt/driveletter Hopefully this article was helpful to those that need it, it’s certainly made my day to day usage on a Windows 10 machine be that much less…well…Windows! If this article was helpful, and you’re looking ","date":"2020-04-19","objectID":"/2020-04-19-wsl-i3-windows/:0:0","tags":["Linux","WSL","i3","i3wm","Windows 10","Windows Subsystem for Linux"],"title":"Running i3 Desktop with WSL on Windows 10","uri":"/2020-04-19-wsl-i3-windows/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"As I have continued my expansion into self-hosting as well as the fediverse, the one challenge I still had was image posting and sharing in an easy and clean looking way. For images on websites like this, especially from a mobile device, FTP uploading has just been inconvenient and disrupts the focused writing activity. I had already dabbled a bit in Pixelfed, by joining Pixelfed.Social when it was still open for registration. This let me test the functionality for a service similar to Instagram or Imgr, but without ads or tracking. The final leap was setting it up self-hosted so that I could fully own that image data. On my Arch VPS that I host on Vultr, I had already tried in previous weeks to direct install Pixelfed. Unfortunately, likely because of other self-hosted apps or packages in place, it was a struggle and I just could not get Pixelfed fully operational. In steps Docker, and the article here that inspired this updated version with what I had to do differently, as well as more details on the Nginx reverse proxy portion. If you are interested in using Vultr to host a VPS for this or any other self-hosted projects, depending on your project size and plans, you can either get $10 of VPS Credit for a small project, or if you’re looking for a larger project, you can get $100 of VPS Credit for 30 days. Both help me out and keep tracking based ads and services off my site. ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:0:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Requirements VPS Operational \u0026 Secure Docker \u0026 Docker-Compose Installed Nginx Installed Domain Name Setup, and Ideally SSL Certificates ready Of note, in this install, Nginx is direct installed on the host, as it fit with my existing environment. It is also common to deploy Nginx with Docker as well, which would change these instructions to some extent. ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:1:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Pixelfed Setup The Pixelfed install will be built directly as a new image, as similar to Jon in the article I referenced, the only pre-built docker images while recent, had no details or notes, so did not inspire the most confidence. Ideally, the steps below should all be run as a non-root user that has Sudo capability. First, a directory to pull the source to, using /opt/ is a good best practice vs. installing it to a specific users /home directory. sudo mkdir -p /opt/pixelfed_source sudo chown $USER:$USER /opt_pixelfed_source Next is pulling down the repository, in my case it’s been active development, so I just pulled the most recent, but you can also pull an older stable build too if you prefer. git clone https://github.com/pixelfed/pixelfed.git /opt/pixelfed_source cd /opt/pixelfed_source git checkout dev This was the first difference I had to work through, where just trying a straight docker build was failing, because in the current versions, the dockerfile files are not in the root, so the command to build has to be updated to reflect that. I also tagged my build with the pull date, given it’s dev so does not have a release number (like v0.10.8) and a commit tag is long and ugly. docker build . -t pixelfed:20200302 -f contrib/docker/Dockerfile.apache There is also a Dockerfile.fpm file there for a php-fpm based version, but as of this writing date, I confirmed it is not complete/functioning yet. Once the docker has built, we can create a directory that the docker compose will run from, which will contain it’s settings, as well as the Pixelfed .env settings file. sudo mkdir -p /opt/pixelfed sudo chown $USER:$USER /opt/pixelfed cd /opt/pixelfed To copy the example configuration file over: cp ../pixelfed_source/.env.example .env nano .env Rather then snippets of key settings, I’m removing sensitive parts and posting up my entire .env config file, as the parts I struggled with the most were the ones where documentation is still a work in progress. Like the article I learned from, I stuck with pgsql within Docker. APP_NAME=\"Pixelfed\" APP_ENV=production APP_KEY=**blank, we'll generate this further down** APP_DEBUG=false APP_URL=https://**your-domain.name** APP_DOMAIN=\"**your-domain.name**\" ADMIN_DOMAIN=\"**your-domain.name**\" SESSION_DOMAIN=\"**your-domain.name**\" TRUST_PROXIES=\"*\" LOG_CHANNEL=stack DB_CONNECTION=pgsql DB_HOST=db DB_PORT=5432 DB_DATABASE=pixelfed DB_USERNAME=pixelfed DB_PASSWORD=**create a proper secure password!** BROADCAST_DRIVER=log CACHE_DRIVER=redis SESSION_DRIVER=redis QUEUE_DRIVER=redis REDIS_SCHEME=tcp REDIS_HOST=redis REDIS_PASSWORD=null REDIS_PORT=6379 MAIL_DRIVER=smtp MAIL_HOST=mailtrap.io MAIL_PORT=587 MAIL_USERNAME=null MAIL_PASSWORD=null MAIL_ENCRYPTION=tls MAIL_FROM_ADDRESS=null MAIL_FROM_NAME=\"Pixelfed\" OPEN_REGISTRATION=false ENFORCE_EMAIL_VERIFICATION=false PF_MAX_USERS=100 MAX_PHOTO_SIZE=64000 MAX_CAPTION_LENGTH=150 MAX_ALBUM_LENGTH=4 MAX_ACCOUNT_SIZE=10000000 IMAGE_QUALITY=100 ACTIVITY_PUB=true AP_REMOTE_FOLLOW=true AP_INBOX=true PF_COSTAR_ENABLED=false HORIZON_EMBED=true ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:2:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Docker / Docker-Compose With that file saved and done, the next step is to setup the docker network and build the docker-compose file for the whole process. docker network create pixelfed docker network create web touch docker-compose.yml nano docker-compose.yml Below is my docker compose file, with sensitive details removed and key points highlighted. Of worthy note in my install, I have host mapped the storage in place of an internal docker volume, to a Fuse S3FS mount point for my S3 storage, which will also make it easier for me to at minimum, backup the raw photo content, and it’s cheaper storage than my VPS SSD. version: '3' services: app: image: pixelfed:20200302 restart: unless-stopped ports: - \"8080:80\" env_file: - ./.env volumes: - \"/path/to/s3-storage:/var/www/storage\" - \"app-bootstrap:/var/www/bootstrap\" - ./.env:/var/www/.env networks: - web - pixelfed db: image: postgres:9.6.4 restart: unless-stopped networks: - pixelfed volumes: - db-data:/var/lib/postgresql/data environment: - POSTGRES_PASSWORD=${DB_PASSWORD} - POSTGRES_USER=${DB_USERNAME} worker: image: pixelfed:20200302 restart: unless-stopped env_file: - ./.env volumes: - \"/path/to/s3-storage:/var/www/storage\" - \"app-bootstrap:/var/www/bootstrap\" networks: - web # Required for ActivityPub - pixelfed command: gosu www-data php artisan horizon redis: image: redis:5-alpine restart: unless-stopped volumes: - \"redis-data:/data\" networks: - pixelfed volumes: redis-data: app-bootstrap: db-data: networks: pixelfed: internal: true web: external: true If you are already running Nginx with other services on the same server, then you may need to adjust the 8080 Port in: Ports: -8080:80 to a different available port, and mirror that change when we get to Nginx shortly. Now we should be able to spin up the image and get it ready for deployment, before we go and sort out Nginx: docker-compose up -d docker-compose exec app php artisan key:generate cat .env | grep APP_KEY # Make sure there's a value Then the container can be restarted, and some final Pixelfed prep tasks complete to have it ready to go. docker-compose restart app docker-compose exec app php artisan config:cache docker-compose exec app php artisan migrate # Answer yes At this point, technically, the Pixelfed instance should be up and running, so we’ll need to sort out the Nginx portion so we can actually access it and verify that fact! ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:3:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Nginx Setup There are a few different methods I’ve seen used for Nginx .conf file locations, but they should all work with minor tweaks. In my case, there is a .conf file for each web application in /etc/nginx/conf.d/nameofapp.conf As such, my base /etc/nginx/nginx.conf is default install, with just these key settings, everything else being commented out or removed. worker_processes 1; error_log logs/error.log; events { worker_connections 1024; } http { include mime.types; include /etc/nginx/conf.d/*.conf; default_type application/octet-stream; sendfile on; keepalive_timeout 65; gzip on; } Then I have the file, /etc/nginx/conf.d/pixelfed.conf setup as below, with placing your correct domain name in place, as well as the path to your certificates. server { listen 80; server_name **your-domain.name**; return 301 https://$server_name$request_uri; client_max_body_size 100M; } server { listen 443 ssl http2; server_name **your-domain.name**; client_max_body_size 100M; ssl_protocols TLSv1.1 TLSv1.2; # ** Adjust below to match your certs, mine were from CloudFlare, but there are plenty of guides for LetsEncrypt, etc. if you're not setting up behind a CDN ** ssl_certificate /etc/nginx/ssl/**your-domain.name**-tld-cert.pem; ssl_certificate_key /etc/nginx/ssl/**your-domain.name**-tld-key.pem; access_log /var/log/nginx/**your-domain.name**.log; location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://127.0.0.1:8080; } } Note the client_max_body_size 100M; lines, I added these after hours digging through PixelFed and PHP.ini type options because I was able to upload pictures, but only if they were under 1MB, and with a very generic failure error. The end solution was that Nginx by default limits file transfers to 1MB if not defined. You can define this here just for Pixelfed, or you can define it globally in the main Nginx.conf file if you prefer. With the pixelfed.conf file generated and filled, hopefully correctly, you should now be able to restart Nginx so that it picks up the file changes. In the case of Arch (and most modern systems), it’s using Systemd, so: sudo systemctl restart nginx If everything at this point has been configured correct, and assuming I have not forgotten any other catch points I ran into at this stage, you should be able to visit https://your-domain.name, and get the default Pixelfed home screen. ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:4:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"User Creation If the site loaded for you, and if like myself you’re building this primarily for private or small group usage, so turned off Open Registration, you can now use the terminal to create you first user, presumably as an admin: docker-compose exec app php artisan user:create That command above will walk you through several requests to build a user, including asking if the user should be an admin (probably yes). Of worthy note, when it asks for Over-ride manual e-mail verification (my wording might be off), enter yes. This marks the accounts e-mail as already verified, so for a small instance you don’t have to focus on e-mail setup right away. With the user created, you should be able to login and start using Pixelfed! ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:5:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Outstanding Issues The only issue I currently am still working on resolving, but it may be caused by a caching network appliance that I can’t test bypassing right away, is collections. On my instance when I try to create a collection, it lets me add from recent or by URL posts to a collection, and even lets me Save the collection. But roughly 75% of the time when I attempt to Publish the collection, I get a generic error message and it does not Publish. When this occurs, my reason I suspect this caching appliance I’m stuck behind, is I can’t even delete the half-created collection, until I wait an hour or so. Selecting delete gives me the prompt, but nothing happens even though Nginx logs show is processed and passed the command through correctly. A handful of times while trying to test this, a collection created perfect without error and published. Once I’ve been able to test this on a direct internet line, I’ll be able to narrow down if I still have a configuration error, and/or if I have an issue I need to submit back to the Github project, given this is the Dev branch. I will also add, I have not yet confirmed if Federation is working for me, but will be validating that when possible, and updating the article if I have to change any settings. ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:6:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Updating Pixelfed This process I am mostly copying direct from the original article I followed, as this install has been recent enough, there have not been any updates yet for me to validate if this creates problems for me. That being said, the commands all look correct, so I don’t anticipate issues. Before updating, especially on Dev branch, it’s a good practice on your VPS to either take a backup or a snapshot just in case. To update to the current Dev: cd /opt/pixelfed_source git checkout dev git pull origin dev git checkout dev docker build . -t pixelfed:todaysdate -f contrib/docker/Dockerfile.apache Then once that has pulled and built, with the updated today’s date to help identify it, it’s just two lines to update in the docker-compose.yml file: cd /opt/pixelfed nano docker-compose.yml app: image: pixelfed:todaysdate #... worker: image: pixelfed:todaysdate Then restart the docker-compose, and run the artisan migrate command. Worthy warning if you’re new to docker, when we say restart, do not run docker-compose down, as that deletes this container and wipes the volume data as well, meaning the next docker-compose up -d would be like a fresh install (without your images or users), so: docker-compose up -d docker-compose exec app php artisan migrate # Answer yes ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:7:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux","Self-Hosted"],"content":"Wrap Up At this point, if everything worked (and I know, it often takes several attempts, this took me a while to get all sorted and running!), you should have a fully functioning PixelFed, and can either open registration, or terminal create users for your own small community to post to. For updates to the Project, keep an eye on Pixelfed GitHub, and as they detail on that page, there is a semi-active group on their Riot Matrix.Org channel for help, which I’ve been over-active in during my install struggles. If this article was helpful and you have not yet settled on a VPS provider, you can help me out by trying Vultr, and we’ll keep that whole mess of ads, data tracking, cookies and the like off this corner of the web! ","date":"2020-03-02","objectID":"/2020-03-02-pixelfed-docker-nginx/:8:0","tags":["Linux","Open Source","Pixelfed","Instagram","Photography","FOSS","Self-Hosted","Docker"],"title":"Pixelfed with Docker and Nginx Reverse Proxy","uri":"/2020-03-02-pixelfed-docker-nginx/"},{"categories":["Tech","Linux"],"content":"Over the last year, I’m progressively been keeping a closer eye on ARM-based systems as they’re growing in power and usability. I currently have a retro game system based on a Raspberry Pi 3B+, and it’s already a great system. As we’re getting into 2020, I’m seeing a lot more articles and posts of the Pi4, at least the 4GB model, making a decent basic workstation and office computer. This is assuming that use case is similar to that of a Chromebook, focused on web and terminal type software. Along the same vein is a system like the Pinebook Pro, which has been getting a fair amount of attention for making the right decisions for a budget system that does not feel budget. I was prompted to post this tonight because I really enjoyed this longer Vice article on the Pinebook Pro and its market in general: The Pinebook Pro, a community-built Linux laptop that runs on ARM hardware, offers a few surprises. Fairly inexpensive, it’s the perfect machine for tinkerers. This $200 Laptop Is Like a Chromebook You Can Hack Currently, getting your hands on a Pinebook Pro at its original price point is challenging. The Pinebook Pro has been popular and not mass-produced, with even lower production volumes for the ISO North American keyboard variant. Knowing that, unless a new batch comes in soon, I’ll likely acquire either a Raspberry Pi4 or a Pine64 - ROCKPro64 for a small home workstation and tinker box for Arch Linux and i3wm. When I do acquire something, I will definitely write about any projects I plan, such as a Plex media server, maybe a small in-home Minecraft box, not to mention a NAS system at some point! ","date":"2020-02-18","objectID":"/2020-02-18-pinebook-pro/:0:0","tags":["Linux","Pine","Pinebook","ARM","Pi"],"title":"ARM Systems and the 2019 Pinebook Pro","uri":"/2020-02-18-pinebook-pro/"},{"categories":["Tech","Linux"],"content":"Among the #self-hosted projects that I run just for my own usage, I have a VPS server running Arch Linux. (Yes, I’m running an Arch server, instead of Ubuntu/Debian) That little VPS runs a few different services: Minecraft server for a group of friends, which is the heavy memory user Write Freely instances for a few subjects Subsonic Music streaming server Resilio Sync encrypted storage target MiniFlux RSS Server/Reader In general, it ticks along without much need for attention. Usually it’s biggest time demands are after each major Minecraft version update (11.x, 12.x, etc.) to adjust settings as it gets more and more resource hungry. FYI: The best solution on 14.x now has been PaperMC Given I work in tech and personally work with Arch Linux plenty, I make sure to run updates on the server weekly, and they almost always are pretty basic updates without much significance. Generally update with my AUR Helper, check the applications, and off I go. This time around this past weekend, but only discovered last night, the updates included an update of PostgreSQL from 11.6 to 12.x. On this server, MiniFlux RSS uses PostgreSQL for it’s backend, and it works well. However, in my past experience I have never had anything use PostgreSQL to have had much awareness about it’s sensitivity in upgrading versions. As such, the update itself did not fail, but when I went to check my Android app which pulls from MiniFlux (via Fever API), it showed it had not pulled for several days. A quick investigation showed that MiniFlux itself did not have an update, but PostgreSQL had indeed updated and the service would not start, according to systemd. A quick check in journalctl -xe, and the source was found as incompatible database versions for PostgreSQL. From that point, it was thankfully not a hard fix, likely because the database for the RSS system is not that large or complex. Referring to the faithful Arch Wiki sent me through a few commands to migrate the data to a backup folder, initialize a new database set under version 12, and then use the pg_upgrade function to upgrade the database to that version. PG_Upgrade did it’s job without fail on the first run, and I was able to then start the PostgreSQL service, followed by a restart of the MiniFlux service. All in all, not a hard process or experience, just a good reminder to keep an eye on packages updating. It’s ideal to be aware of what you are updating, but sometimes an innocent looking update breaks things. When it’s a hobby server, keep good backups and be willing to research the fix, and you’ll learn a bit along the way. I’m not sure what my next project will be to add to my #Self-Hosted systems, but I’ve started catching up on the Self-Hosted Podcast from Jupiter Broadcasting, so I’m sure I’ll discover something new! If you don’t yet #Self-Host and are looking for a project, I run my projects currently on Vultr, and sticking with plain direct referral link, you can help me out by giving Vultr a try with $50 worth of hosting, and it helps me out as well. I have a About / Privacy page linked at the top where I talk about my privacy rules: Text only referral links, no cookies/trackers, no ugly banners or graphics interrupting the writing. ","date":"2019-11-27","objectID":"/2019-11-27-arch-server-updates/:0:0","tags":["Linux","Arch","Arch Linux","MiniFlux","PostgreSQL"],"title":"Arch Linux: Blind Updating, PostgreSQL and MiniFlux","uri":"/2019-11-27-arch-server-updates/"},{"categories":["Ramble, Rant and Musings"],"content":"My writing here is mostly focused on technology and gaming. Being a general geek into all sorts of subjects and thought, sometimes an idea just sticks in your head and needs to be written. I don’t know the genesis of the idea forming in my head, probably watching sci-fi shows too late, but my mind starting musing on the concept of time. Specifically, how we measure it, account for it, and place meaning on it despite a limited view of it. The idea in my head when I’m stating limited view of time is focused on the fact that every method we use to measure time is relative. It’s relative to the rotation of the earth, relative to our orbit around our sun. All the significance we place on it’s passage is not measured the same without this relative constant of earth. An example to elaborate this can focus on something as core to most of us as our age. I am nearing my thirty fifth birthday, but that is still an arbitrary number based on when I was born followed by thirty five rotations of the earth around our sun. What if as humanity expands I was born and raised on Mars? Following the same relative basis I would be only half way through my eighteenth year of existence! So does my age as a number really mean anything? One could attempt to make the argument that humanity as we know it originates from earth, and as such, any expansion we would logically still keep time based on our home world. But would we? Carry on the Mars thought. What if we’re in a situation like The Expanse and Earth and Mars do not really get along, and have fought wars in the past to form very different cultures. Why would anyone born on Mars measure time as it occurs on Earth that they’ve never been too and maybe even have conflicts with? The concept and thoughts are still developing and expanding even as I write, and I expect will continue to do so. The argument holds for all measurements of time that we use. The length of a day is based on the earths rotation, and in near expansion, Mars is similar, other planets a day may be longer or shorter. The division of a day into smaller and smaller units from hours to minutes is an arbitrary and universally agreed to standard simply invented by powerful governments or religions through history. In summary, at least for now, all of time is measured only be relative means. I’m certain quantum science or other think tank machines have and continue to dissect these concepts in significant detail beyond my comprehension. That however does not preclude my own exploration on the subject. Maybe time really is just wibbly-wobbly, timey-wimey stuff… ","date":"2019-09-05","objectID":"/2019-09-05-timeconcepts/:0:0","tags":["Ramble","Time","Space","Random","Thinking","Musings","Science"],"title":"Concepts of Time","uri":"/2019-09-05-timeconcepts/"},{"categories":["Tech","Linux"],"content":"When I first really transitioned into the Linux world from Windows, I started with Ubuntu. That is a very common introductory point for many, or was, and from there I was an aggressive distro-hopper. I shifted through the Kubuntu’s and Xubuntu’s of the day, into ArchLabs on one laptop. At some point about two years ago took a stop at elementary OS in it’s 0.4 version (just one version before 5, they dropped the decimals). When I first ran elementary OS 0.4, I liked the style and cohesive design. Compared to some of the distros I had tried it took some of that raw linux feel away, as far as what a normal end user might experience. Overall, I liked it and was going to install it on my main system. The full stop point I hit was the distro was based on Ubuntu 16.04 LTS. The kicker with that was the system I was using was UEFI only, and the linux kernel for 16.04 LTS did not yet natively support UEFI, nor had I discovered tools like ReFind. As such, I continued on hopping until falling into my current Arch Linux and i3wm realm, which I’ll talk on seperatly. Forward to this past week, and I was sorting our a personal laptop for a co-worker that was collecting dust with a very disliked Windows 10 install. As a very non-tech user, I convinved him I could make the system run way better, and for his mostly browser based usage, be easier to use as well. My target was the clean looking elementary OS. The install on the roughly two year old HP laptop went quick and smooth. The only mandatory post-install task that was a bit convuluted was getting to an option to enable restricted/proprietary drivers to get the broadcom wireless to work. I found Juno was still new enough that searching online was giving a fair volume of 0.3 and 0.4 version answers which did not directly apply to version 5. With the operating system looking even cleaner and buttoned together, and now based on 18.04 LTS Ubuntu, I decided it was time to give it a try on my little travel laptop, my Cube i7 Book. (Artwork By: Scott Johnson of FrogPants Network As in the past, the install was quick and easy. The installer continues to be cleaned up and simplified that it’s honestly more confusing to fresh install Windows 10 than elementary OS. (Mostly because Windows is less informative, if at all, on the disk partition/selection screen). Additionally, the options which I took advantage of to have full drive encryption were readily available and easy to setup during the install process. Once the install was completed a few minutes later, the core system was up and running, and as in the previous version, makes a great first impression. Even for an oddball system like this Cube i7 Book, all the hardware was detected and setup, inlcuding WiFi and BlueTooth. For a large majority of users, at this point the system is ready for day to day usage, with a browser installed, ability to view photos and videos, setup your e-mail provider and listen to some music. Beyond that, the AppCenter is easy to navigate with both curated and non-curated sections to cover most users needs. I also like seeing the Pay What You Want model to the AppCentre, that allows you to support the developers if you can, but also try out the software before you commit. In my searching the AppCentre, I also found a few applications that are technically exlusive to Elementary OS that I really liked. I’m hesitant to use the word exclusive given the nature of open source, but in it’s spirit, these apps can still be installed from source/Git with the associated dependencies on other distros. For my usage I came across Ephemeral, a good privacy web browser option with isolated windows, no history tracking, and ads/trackers blocked by it’s nature (no over-ride options). My second finding that is a nice draw to elementary was WebPin. A simple application for creating attractive shortcuts to websites as apps, running in a simple minimal window. While this function could mostly be obtained with a quickly crafted .Desktop file,","date":"2019-08-05","objectID":"/2019-08-02-elementary-5-review/:0:0","tags":["Linux","Elementary","Elementary OS","OS Review","Juno"],"title":"Elementary OS 5 (Juno) Review","uri":"/2019-08-02-elementary-5-review/"},{"categories":["Tech","Linux"],"content":"Why couldn’t I just be fine with Nano and Sublime Text? I guess I can’t help but be curious! Up until a few days ago, my previous experience with VIM was limited. I’ve been using Linux heavily for years now, and it was just an annoyance. In the past my only exposure to VIM was AUR install scripts or source builds that forced me to check build files via VIM. What that really meant was it was that annoying text program, that I had no clue how to quit when it suddenly was on my screen! :w to save, and :q to quit, but not if your in insert mode…what? Well, now that’s changing, progressively. I believe the seed for it started at some point in my last work rotation, where for some reason my brain said I should go read a how-to article on VIM. I read the first blurb about moving around a document, and going into insert mode, and it mostly seemed unnecessary on its own. When I started to read combining commands together, like 5w to move five words down, ( and ) for moving sentences and more…it started to click. As VIM' own site says, they don’t hold hands. It appears to be a very powerful text editor, but you have to learn the commands and try them out in your own workflow. So I studied a few more pages on VIM tutorials and understanding it’s powers. Then, I became curious if it could support MarkDown for writing, and sure enough, VIM-MarkDown plugin…easy! How could I paste from the web into VIM? In my case at least, Shift + Left/Right Mouse Together, given a lack of middle mouse on the laptop. The end result of all this? Well, this post was written in VIM with MarkDown syntax highlighting. It was a very distraction free writing environment. Finally, even at my novice level, it was easy to move around the document and get familiar with the commands. It’s not hard to see that it has a steep learning curve, but especially on longer articles and research notes, how easy it can make working with text. I am not a committed VIM user yet, but this is definitely an extended expedition into foreign lands, and other than the confusing signs, is quite intriguing and worth further exploration. ","date":"2019-05-08","objectID":"/2019-05-08-vim/:0:0","tags":["Linux","Plain Text","Markdown","VIM","Tech","Text","Writing"],"title":"VIM Expedition","uri":"/2019-05-08-vim/"},{"categories":["Tech","Gaming"],"content":"I have continued to play Elder Scrolls: Blades, mainly on my iPhone for the past month in bursts. Overall, it is still holding up, although obviously it is not remotely like a full Elder Scrolls PC game. I found I had very little trouble through the single digits and into the teen levels. The occasional level had some minor grind spots for difficulty, but nothing too bad. The early 20’s levels however, started off pretty rough. Granted, this was in part because around level 19, I chose to re-spec skills towards light weapons and magic. Learning curve with magic converting to it later in the game was a bit harder, so more enemies beat me around. The real sweet spot from my current play came around level 23-24, when most of this gear combo below came together. It turned out to be an amazing gear set all the way to level 29. The two pieces of gear that received bonus enchantment of extra healing effects were key. With that extra healing, food in missions healed a lot. With the third piece being the gauntlets, I was regularly doing 4-5 Skull missions without issues, as long as I kept dealing damage I could out-heal my opponent. Finally, I generally combined that with the basic Absorb spell: Which would cover the minor gaps in healing occasionally needed if I had a round of good blocks from the enemy killing my combo or damage output. The rest of the fighting was mostly combo damage with a weapon matched to the opponent, and the occasional spell with the right damage type. As I continue playing, my next grind and really challenging point was when I hit level 29. At twenty-nine, all the sudden the difficulty jumped and I was struggling to out heal the damage a lot of opponents could deal. For the most part I simply had to slow down my pace of fighting and be a bit more defensive then I was being. Now, I’m sitting at level 31, and my newest gear that is chewing through opponents has been a Glass Dagger, enchanted with the bonus combo damage. Previously all my weapons were elemental damage, but with a fast weapon, that bonus combo damage is laying waste even to enemies resistant to slashing damage! Beyond that, my town is coming along nicely, my various production buildings are between level five and seven. I’m just starting to work into Glass and Charus gear, and already eyeing beyond it towards Ebony with the level eight smithy. I’m curious what the long term future of the game will be, if the Arena will turn it into an aggressive pay-to-win formula, or if they’ll be able to add more exploration and story element so that the end-game is not just Arena matches. ","date":"2019-04-29","objectID":"/2019-04-29-blades-20s-gear/:0:0","tags":["Elder Scrolls","Bethesda","Gaming","Mobile"],"title":"Elder Scrolls: Blades - The 20's and Great Gear","uri":"/2019-04-29-blades-20s-gear/"},{"categories":["Tech","Gaming"],"content":"As many will have seen, the originally announced last year mobile Elder Scrolls game has finally landed! Granted, they’ve labelled it as Early Access and are slowly rolling out invites to iOS and Android Users. That being said, I was invited a little over forty-eight hours ago. ","date":"2019-04-02","objectID":"/2019-04-02-bladesearlyaccess/:0:0","tags":["Elder Scrolls","Blades","Gaming","Mobile"],"title":"Elder Scrolls: Blades - Early Access","uri":"/2019-04-02-bladesearlyaccess/"},{"categories":["Tech","Gaming"],"content":"In Brief It’s really good. It looks absolutely gorgeous on my iPhone 8, and only slightly less so on my older iPad Air 2. Character design is more flexible than most mobile games, and starting out, it does actually have a story. Unfortunately, it is Free to Play model, but it feels fine. It’s definitely worth a try for Elder Scrolls on the go! ","date":"2019-04-02","objectID":"/2019-04-02-bladesearlyaccess/:0:1","tags":["Elder Scrolls","Blades","Gaming","Mobile"],"title":"Elder Scrolls: Blades - Early Access","uri":"/2019-04-02-bladesearlyaccess/"},{"categories":["Tech","Gaming"],"content":"Game-play The controls are perfect! It’s better by a long shot in portrait mode, where it’s all one hand, one finger really, playable. The excellent execution of the controls really helps make the game. Free roaming around your own town is vivid and alive, and your going to be rebuilding it to your style. The story is off to a good start, I expect they will keep fleshing it out as the game leaves Early Access. Outside of the story there are always quick quests to do to find better gear and resources for your town. It’s not full world free-roaming like a full PC Elder Scrolls game, but it’s also not static menu’s or walk-on-rails. ","date":"2019-04-02","objectID":"/2019-04-02-bladesearlyaccess/:0:2","tags":["Elder Scrolls","Blades","Gaming","Mobile"],"title":"Elder Scrolls: Blades - Early Access","uri":"/2019-04-02-bladesearlyaccess/"},{"categories":["Tech","Gaming"],"content":"Visuals Nothing to say, it looks better than I expected, so here is a trio of shots that are at least Oblivion level, I think HD Oblivion texture level, at least on iPhone 8. ","date":"2019-04-02","objectID":"/2019-04-02-bladesearlyaccess/:0:3","tags":["Elder Scrolls","Blades","Gaming","Mobile"],"title":"Elder Scrolls: Blades - Early Access","uri":"/2019-04-02-bladesearlyaccess/"},{"categories":["Tech","Gaming"],"content":"What I Want Next? More Quest Text: Flesh out major story, and add more story to minor/job quests. Bethesda’s writers are well known, so I expect they’ll apply this as the game expands. Additional Free Roam: I like what it has now, as the game grows I expect they’ll add more options to customize the town further, and I would like to maybe have a small area outside the town walls to free roam. I think this is likely if the game does well for Bethesda. ","date":"2019-04-02","objectID":"/2019-04-02-bladesearlyaccess/:0:4","tags":["Elder Scrolls","Blades","Gaming","Mobile"],"title":"Elder Scrolls: Blades - Early Access","uri":"/2019-04-02-bladesearlyaccess/"},{"categories":["Tech","Gaming"],"content":"What I Dislike? Unfortunately, it is a “Free to Play” model, but it’s not too invasive. I have not seen any dreaded advertisements or “watch ads to do X faster” options, so kudo’s to that decision. I will likely do some small purchases to support Bethesda, as the F2P has not been in my face or prompting in any way. There you have it, I think Elder Scrolls: Blades is off to a good start with the Early Access tag. It has some good potential, looks great, didn’t murder my battery life and the Free to Play model is not invasive at all and has no advertisements. ","date":"2019-04-02","objectID":"/2019-04-02-bladesearlyaccess/:0:5","tags":["Elder Scrolls","Blades","Gaming","Mobile"],"title":"Elder Scrolls: Blades - Early Access","uri":"/2019-04-02-bladesearlyaccess/"},{"categories":["Tech","Self-Hosted"],"content":"The last few evenings after work I have kept very occupied as I tackled a variety of #SelfHosting projects on my plate. If you visited this site at all in the past year, it was still a very busy ‘news magazine’ looking WordPress site. It liked it at the time. I am shifting more to quick, personal writing on my tech adventures. Hence, TechZerker.com has moved to the reading focused WriteFreely platform. It’s open source, light on server resources, and was a breeze to implement! It is the project that powers the more well known Write.As platform. While I was at it, I also moved my cloud storage system from it’s existing VPS to this new Vultr VPS. I have been using Resilio Sync to that end for nearly a year now. It has been working great for my needs and I will do a better write-up on it, and it’s installation. Last on the list, and easier than I expected, I deployed MiniFlux to replace Feedly. I continue to prefer a curated RSS feed over what social media services try to serve me. I had one or two small bumps with steps not really documented, but they were simple to figure out! All those services are now up and running, and are now on my preferred pure Arch Linux distro. The last project on the plate is to migrate an existing MineCraft. For that project I’m going to test out MineOS to simplify the management. I will do additional write-ups or commentary on each of the applications when I am ready to write about them. In the meantime, enjoy the cleaner and way faster loading TechZerker! –Scott ","date":"2019-03-24","objectID":"/2019-03-24-self-host-migration/:0:0","tags":["Linux","Self-Hosted","Miniflux"],"title":"Self Hosting Migration Rush","uri":"/2019-03-24-self-host-migration/"}]